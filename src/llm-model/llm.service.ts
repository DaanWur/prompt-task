import { Injectable } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import path from 'path';

@Injectable()
export class LlmService {
  private llama: any;
  private model: any;
  private context: any;
  private session: any;

  constructor(private readonly configService: ConfigService) {}

  /**
   * Initializes the module by loading the Llama model and creating a context and session.
   * This method is called automatically when the module is initialized.
   */
  private async onModuleInit() {
    const { getLlama, LlamaChatSession } = await this.myLogic();
    this.llama = await getLlama();
    this.model = await this.llama.loadModel({
      modelPath: path.join(this.configService.getOrThrow<string>('MODEL_PATH')),
    });
    this.context = await this.model.createContext();
    this.session = new LlamaChatSession({
      contextSequence: this.context.getSequence(),
    });
  }

  /**
   * Dynamically imports the 'node-llama-cpp' module and extracts the necessary components.
   * @returns An object containing the getLlama function and LlamaChatSession class.
   */
  private async myLogic() {
    const nlc: { getLlama: any; LlamaChatSession: any } = await Function(
      'return import("node-llama-cpp")',
    )();
    const { getLlama, LlamaChatSession } = nlc;
    return { getLlama, LlamaChatSession };
  }

  /**
   * Generates a response from the Llama model based on the provided message.
   * @param message - The input message to prompt the Llama model.
   * @returns The response generated by the Llama model.
   * @throws An error if the response generation fails.
   */
  async generateResponse(message: string) {
    try {
      const response = await this.session.prompt(message);
      return response;
    } catch (error) {
      const message = `Error: ${error.message}, Stack: ${error.stack}, Code: ${error.code}`;
      throw new Error(message);
    }
  }
}
